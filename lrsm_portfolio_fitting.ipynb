{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import strat_models\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from risk_return_models import *\n",
    "from utils import *\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"df_train.csv\", index_col=\"Date\")\n",
    "df_val = pd.read_csv(\"df_test.csv\", index_col=\"Date\")\n",
    "df_holdout = pd.read_csv(\"df_holdout.csv\", index_col=\"date\")\n",
    "\n",
    "_, num_assets = df_train.shape\n",
    "\n",
    "Z_train = pd.read_csv(\"Z_train.csv\", index_col=\"Date\")\n",
    "Z_val = pd.read_csv(\"Z_test.csv\", index_col=\"Date\")\n",
    "Z_holdout = pd.read_csv(\"Z_holdout.csv\", index_col=\"Date\")\n",
    "\n",
    "num_quantiles = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_mean = get_data_dict(df_Y=df_train, df_Z=Z_train, num_assets=num_assets)\n",
    "val_mean = get_data_dict(df_Y=df_val, df_Z=Z_val, num_assets=num_assets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#here, we fit the stratified return model.\n",
    "kwargs = dict(verbose=True, abs_tol=1e-6, maxiter=2000, rho=10)\n",
    "\n",
    "M, local, w1, w2, w3, w4 = 0.01, 0.0075, 5, 10, 1000, 1000\n",
    "                            \n",
    "print(M, local, w1, w2, w3, w4)\n",
    "\n",
    "G_vix = nx.path_graph(num_quantiles) #vix quantiles (deciles)\n",
    "G_unemp = nx.path_graph(num_quantiles) #volume quantiles\n",
    "G_inflation = nx.path_graph(num_quantiles) #volume quantiles\n",
    "G_mort = nx.path_graph(num_quantiles) #volume quantiles\n",
    "\n",
    "strat_models.set_edge_weight(G_vix, w1)\n",
    "strat_models.set_edge_weight(G_unemp, w2)\n",
    "strat_models.set_edge_weight(G_inflation, w3)\n",
    "strat_models.set_edge_weight(G_mort, w4)\n",
    "\n",
    "G = strat_models.cartesian_product([G_vix, G_unemp, G_inflation, G_mort])\n",
    "\n",
    "loss = huber_return_loss(M=M)\n",
    "reg = strat_models.sum_squares_reg(lambd=local)\n",
    "\n",
    "bm = strat_models.BaseModel(loss=loss,reg=reg)\n",
    "sm = strat_models.StratifiedModel(BaseModel=bm, graph=G)\n",
    "\n",
    "sm.fit(data=train_mean, **kwargs)\n",
    "\n",
    "preds_train = np.vstack([\n",
    "    sm.G._node[tuple(Z_train.loc[date].values)][\"theta\"] for date in Z_train.index])\n",
    "\n",
    "preds_val = np.vstack([\n",
    "    sm.G._node[tuple(Z_val.loc[date].values)][\"theta\"] for date in Z_val.index])\n",
    "\n",
    "sm_corr_train = CORR_SM(preds=preds_train, df=df_train)\n",
    "sm_corr_val = CORR_SM(preds=preds_val, df=df_val)\n",
    "\n",
    "print(\"Stratified return model correlations:\")\n",
    "print(\"\\ttrain = {}\".format(sm_corr_train))\n",
    "print(\"\\tval = {}\".format(sm_corr_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Here, we fit the common model.\n",
    "common_mean = df_train.mean(axis=0)\n",
    "    \n",
    "preds_train = np.vstack([common_mean for date in Z_train.index])\n",
    "preds_val = np.vstack([common_mean for date in Z_val.index])\n",
    "\n",
    "corr_train = CORR_SM(preds=preds_train, df=df_train)\n",
    "corr_val = CORR_SM(preds=preds_val, df=df_val)\n",
    "\n",
    "print(\"Common return model correlations:\")\n",
    "print(\"\\t train = {}\".format(corr_train))\n",
    "print(\"\\t val = {}\".format(corr_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "means = dict()\n",
    "for node in sm.G.nodes():\n",
    "    means[node] = sm.G._node[node][\"theta\"].copy()\n",
    "    \n",
    "rets = pd.DataFrame(data=np.vstack([means[key] for key in means.keys()]), columns=df_train.columns)\n",
    "\n",
    "tab = rets.describe().loc[[\"50%\", \"min\", \"max\"]].rename(index={\"50%\":\"median\"}).T\n",
    "tab[\"common\"] = common_mean\n",
    "tab = tab[[\"common\", \"median\", \"min\", \"max\"]] * 100\n",
    "\n",
    "print(tab.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#putting covariance data in form to be accepted by stratified model.\n",
    "\n",
    "train_cov = get_data_dict(df_Y=df_train, df_Z=Z_train, num_assets=num_assets)\n",
    "val_cov = get_data_dict(df_Y=df_val, df_Z=Z_val, num_assets=num_assets)\n",
    "\n",
    "for i in range(len(train_cov[\"Y\"])):\n",
    "    if not np.allclose(train_cov[\"Y\"][i], 0):\n",
    "        train_cov[\"Y\"][i] = (train_cov[\"Y\"][i])*100\n",
    "for i in range(len(val_cov[\"Y\"])):\n",
    "    if not np.allclose(val_cov[\"Y\"][i], 0):\n",
    "        val_cov[\"Y\"][i] = (val_cov[\"Y\"][i])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"STRATIFIED MODEL\"\"\"\n",
    "#here, we fit the stratified risk model.\n",
    "kwargs = dict(verbose=True, abs_tol=1e-6, maxiter=2000, rho=10)\n",
    "\n",
    "w1, w2, w3, w4 = 1, 0.2, 100, 50\n",
    "\n",
    "G_vix = nx.path_graph(num_quantiles) #vix quantiles (deciles)\n",
    "G_unemp = nx.path_graph(num_quantiles) #volume quantiles\n",
    "G_inflation = nx.path_graph(num_quantiles) #volume quantiles\n",
    "G_mort = nx.path_graph(num_quantiles)\n",
    "\n",
    "strat_models.set_edge_weight(G_vix, w1)\n",
    "strat_models.set_edge_weight(G_unemp, w2)\n",
    "strat_models.set_edge_weight(G_inflation, w3)\n",
    "strat_models.set_edge_weight(G_mort, w4)\n",
    "\n",
    "G = strat_models.cartesian_product([G_vix, G_unemp, G_inflation, G_mort])\n",
    "\n",
    "loss = covariance_max_likelihood_loss()\n",
    "reg = strat_models.trace_reg(lambd=0)#we include no regularization (lambd=0).\n",
    "\n",
    "bm = strat_models.BaseModel(loss=loss,reg=reg)\n",
    "sm = strat_models.StratifiedModel(BaseModel=bm, graph=G)\n",
    "\n",
    "sm.fit(data=train_cov, **kwargs)\n",
    "\n",
    "covs = dict() \n",
    "for node in sm.G.nodes():\n",
    "    covs[node] = np.linalg.inv(sm.G._node[node][\"theta\"].copy()).copy()\n",
    "    \n",
    "print(\"Stratified risk model losses:\")\n",
    "print(\"\\ttrain = {}\".format(sm.anll(train_cov)))\n",
    "print(\"\\tval = {}\".format(sm.anll(val_cov)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Common model\n",
    "#Here, we just put the empirical covariance matrix into a stratified model class\n",
    "#so we can invoke the ANLL functions to compare the common model to the \n",
    "#stratified model.\n",
    "\n",
    "theta_common = (df_train*100).cov().values\n",
    "\n",
    "G_vix = nx.path_graph(num_quantiles) #vix quantiles (deciles)\n",
    "G_unemp = nx.path_graph(num_quantiles) #volume quantiles\n",
    "G_inflation = nx.path_graph(num_quantiles) #volume quantiles\n",
    "G_mort = nx.path_graph(num_quantiles)\n",
    "\n",
    "G = strat_models.cartesian_product([G_vix, G_unemp, G_inflation, G_mort])\n",
    "\n",
    "loss = covariance_max_likelihood_loss()\n",
    "reg = strat_models.trace_reg(lambd=local)\n",
    "\n",
    "bm_common = strat_models.BaseModel(loss=loss,reg=reg)\n",
    "sm_common = strat_models.StratifiedModel(BaseModel=bm_common, graph=G)\n",
    "\n",
    "for node in sm.G.nodes():\n",
    "    sm_common.G._node[node][\"theta\"] = np.linalg.inv(theta_common)\n",
    "    sm_common.G._node[node][\"theta_tilde\"] = np.linalg.inv(theta_common)\n",
    "    sm_common.G._node[node][\"theta_hat\"] = np.linalg.inv(theta_common)\n",
    "    \n",
    "print(\"train:\", sm_common.anll(train_cov))\n",
    "print(\"validation:\", sm_common.anll(val_cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common_vols = np.sqrt((100*df_train).cov().values.diagonal()/(100*100))\n",
    "\n",
    "vols = pd.DataFrame(data=np.vstack([np.sqrt(covs[key].diagonal()/(100*100)) for key in covs.keys()]), \n",
    "                    columns=df_train.columns)\n",
    "\n",
    "tab = vols.describe().loc[[\"50%\", \"min\", \"max\"]].rename(index={\"50%\":\"Median\"}).T\n",
    "tab[\"Common\"] = common_vols\n",
    "tab = tab[[\"Common\", \"Median\", \"min\", \"max\"]]\n",
    "\n",
    "print((tab*100).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assets = df_train.columns\n",
    "VTI_idx = np.where(assets==\"VTI\")[0][0]\n",
    "\n",
    "common_corrs = pd.DataFrame(data=correlation_from_covariance(df_train.cov().values)[VTI_idx].reshape(-1,1),\n",
    "                            index=assets,\n",
    "                            columns=[\"Common\"])\n",
    "\n",
    "corrs_strat = []\n",
    "for key in covs.keys():\n",
    "    corr_mtx = correlation_from_covariance(covs[key])\n",
    "    corrs_strat += [corr_mtx[VTI_idx]]\n",
    "\n",
    "corrs = pd.DataFrame(data=np.vstack(corrs_strat),\n",
    "                    columns=df_train.columns)\n",
    "\n",
    "tab = corrs.describe().loc[[\"50%\", \"min\", \"max\"]].rename(index={\"50%\":\"Median\"}).T\n",
    "tab[\"Common\"] = common_corrs\n",
    "\n",
    "tab = tab[[\"Common\", \"Median\", \"min\", \"max\"]]\n",
    "\n",
    "print((tab).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For the backtest on the held-out data, \n",
    "#we retrain on all of the training and validation data.\n",
    "#Those models are in these .pkl files.\n",
    "\n",
    "means_file = open('means.pkl', 'rb')\n",
    "means = pickle.load(means_file)\n",
    "means_file.close()\n",
    "\n",
    "common_means_file = open('common_means.pkl', 'rb')\n",
    "common_means = pickle.load(common_means_file)\n",
    "common_means_file.close()\n",
    "\n",
    "covs_file = open('covs.pkl', 'rb')\n",
    "covs = pickle.load(covs_file)\n",
    "covs_file.close()\n",
    "\n",
    "common_inv_cov_file = open('common_inv_cov.pkl', 'rb')\n",
    "common_inv_cov = pickle.load(common_inv_cov_file)\n",
    "common_inv_cov_file.close()\n",
    "\n",
    "tau = pd.read_csv(\"tau.csv\", index_col=\"TICKER\", squeeze=True)\n",
    "kappa = pd.read_csv(\"kappa.csv\", index_col=\"TICKER\", squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "portfolio_values, df_Zs, W, sharpes_strat, returns_strat = backtest(returns=df_holdout.values,\n",
    "                                            Z_returns=Z_holdout,\n",
    "                                            benchmark=df_holdout[\"VTI\"],\n",
    "                                            means=means, covs=covs,\n",
    "                                            lev_lim=2,\n",
    "                                            bottom_sec_limit=-0.25, \n",
    "                                            upper_sec_limit=0.3,\n",
    "                                            shorting_cost=2.37,\n",
    "                                            tcost=0.32,\n",
    "                                            MAXRISK=2.5e-5,\n",
    "                                            tau=tau.values,\n",
    "                                            kappa=kappa.values)\n",
    "\n",
    "portfolio_values_common, Wcommon, sharpes_common = backtest_common(returns=df_holdout.values,\n",
    "                                            Z_returns=Z_holdout,\n",
    "                                            benchmark=df_holdout[\"VTI\"],\n",
    "                                            mean = common_means[(0,0,0,0)],\n",
    "                                            cov = common_inv_cov,\n",
    "                                            lev_lim=2,\n",
    "                                            bottom_sec_limit=-0.25,\n",
    "                                            upper_sec_limit=0.3,\n",
    "                                            shorting_cost=1.33,\n",
    "                                            tcost=0.075,\n",
    "                                            MAXRISK=2.5e-5,\n",
    "                                            tau=tau.values,\n",
    "                                            kappa=kappa.values)\n",
    "\n",
    "portfolio_values[\"Common model policy\"] = portfolio_values_common[\"Common model policy\"]\n",
    "\n",
    "print(\"SHARPE OF STRATIFIED MODEL POLICY:\", sharpes_strat[\"Stratified model policy\"])\n",
    "print(\"SHARPE OF COMMON MODEL POLICY:\", sharpes_common[\"Common model policy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, figsize=(10,8), sharex=True)\n",
    "\n",
    "mapping = {\"vix\":\"Volatility\", \"unemp\":\"Unemployment\", \"inf\":\"Inflation\", \"mort\":\"Mortgage\"}\n",
    "df_Zs = df_Zs.rename(columns=mapping)\n",
    "\n",
    "(1+df_Zs).plot(ax=ax[0], color=[\"purple\", \"orange\", \"gold\", \"red\"], fontsize=\"x-large\",\n",
    "              yticks=range(1,11))\n",
    "\n",
    "portfolio_values = portfolio_values.rename(columns={\"benchmark\":\"VTI\"})\n",
    "portfolio_values.plot(ax=ax[1], color=[\"black\", \"blue\", \"red\"], fontsize=\"x-large\")\n",
    "\n",
    "ax[0].set_xlabel(\"Date\", fontsize=\"x-large\")\n",
    "ax[0].set_ylabel(\"Economic conditions decile\", fontsize=\"x-large\")\n",
    "ax[1].set_xlabel(\"Date\", fontsize=\"x-large\")\n",
    "ax[1].set_ylabel(\"Portfolio value\", fontsize=\"x-large\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(10,8), sharex=True)\n",
    "\n",
    "W[0] = W[0].reshape(-1,1)\n",
    "Wcommon[0] = Wcommon[0].reshape(-1,1)\n",
    "\n",
    "pd.DataFrame(np.hstack(W).T[1:,:], index=portfolio_values.index[1:]).plot(ax=ax[0], \n",
    "                                                                          fontsize=\"x-large\", \n",
    "                                                                          legend=False)\n",
    "pd.DataFrame(np.hstack(Wcommon).T[1:,:], index=portfolio_values.index[1:]).plot(ax=ax[1], \n",
    "                                                                                fontsize=\"x-large\", \n",
    "                                                                                legend=False)\n",
    "\n",
    "ax[0].set_ylabel(\"Stratified model\\nholdings\", fontsize=\"x-large\")\n",
    "ax[0].set_xlabel(\"Date\", fontsize=\"x-large\")\n",
    "ax[1].set_ylabel(\"Common model\\nholdings\", fontsize=\"x-large\")\n",
    "ax[1].set_xlabel(\"Date\", fontsize=\"x-large\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def annualized_return_risk(portfolio_values):\n",
    "    \n",
    "    portfolio_values = portfolio_values.pct_change()\n",
    "    P = 250\n",
    "\n",
    "    ann_return = portfolio_values.mean()*P\n",
    "    ann_risk = portfolio_values.std()*np.sqrt(P)\n",
    "    \n",
    "    return ann_return, ann_risk\n",
    "\n",
    "ret_sm, risk_sm = annualized_return_risk(portfolio_values=portfolio_values[\"Stratified model policy\"])\n",
    "ret_common, risk_common = annualized_return_risk(portfolio_values=portfolio_values[\"Common model policy\"])\n",
    "ret_bmark, risk_bmark = annualized_return_risk(portfolio_values=portfolio_values[\"VTI\"])\n",
    "\n",
    "portfolio_returns = portfolio_values.pct_change().dropna()\n",
    "alphas = portfolio_returns.subtract(portfolio_returns[\"VTI\"],axis=0)\n",
    "IR = np.sqrt(250)*alphas.mean() / alphas.std()\n",
    "\n",
    "\n",
    "print(\"\\t\\tANNUALIZED\")\n",
    "print(\"\\t\\tRETURN          RISK               SHARPE        IR\")\n",
    "print(\"Stratified\\nmodel:\")\n",
    "print(\"\\t\", ret_sm, risk_sm, ret_sm/risk_sm, IR[\"Stratified model policy\"])\n",
    "print(\"Common\\nmodel:\")\n",
    "print(\"\\t\", ret_common, risk_common, ret_common/risk_common, IR[\"Common model policy\"])\n",
    "print(\"Benchmark:\")\n",
    "print(\"\\t\", ret_bmark, risk_bmark, ret_bmark/risk_bmark, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fs = [\"mktrf\", \"smb\", \"hml\", \"umd\"]\n",
    "factors = pd.read_csv(\"fama_french_factors.csv\", index_col=\"date\")\n",
    "factors.index = pd.to_datetime(factors.index)\n",
    "factors[\"alpha\"] = 1\n",
    "Y = portfolio_values[\"Stratified model policy\"].pct_change().dropna()\n",
    "Y.index = pd.to_datetime(Y.index)\n",
    "X = factors.loc[Y.index,fs+[\"alpha\"]]\n",
    "\n",
    "strat_ff = np.linalg.inv(X.T@X)@X.T@Y\n",
    "strat_ff = pd.DataFrame(strat_ff.values.reshape(1,-1), columns=fs+[\"alpha\"], index=[\"Stratified model policy\"]).T\n",
    "\n",
    "factors = pd.read_csv(\"fama_french_factors.csv\", index_col=\"date\")\n",
    "factors.index = pd.to_datetime(factors.index)\n",
    "factors[\"alpha\"] = 1\n",
    "Y = portfolio_values[\"Common model policy\"].pct_change().dropna()\n",
    "Y.index = pd.to_datetime(Y.index)\n",
    "X = factors.loc[Y.index,fs+[\"alpha\"]]\n",
    "\n",
    "common_ff = np.linalg.inv(X.T@X)@X.T@Y\n",
    "common_ff = pd.DataFrame(common_ff.values.reshape(1,-1), columns=fs+[\"alpha\"], index=[\"Common model policy\"]).T\n",
    "\n",
    "factors = pd.read_csv(\"fama_french_factors.csv\", index_col=\"date\")\n",
    "factors.index = pd.to_datetime(factors.index)\n",
    "factors[\"alpha\"] = 1\n",
    "Y = portfolio_values[\"VTI\"].pct_change().dropna()\n",
    "Y.index = pd.to_datetime(Y.index)\n",
    "X = factors.loc[Y.index,fs+[\"alpha\"]]\n",
    "\n",
    "bmark_ff = np.linalg.inv(X.T@X)@X.T@Y\n",
    "bmark_ff = pd.DataFrame(bmark_ff.values.reshape(1,-1), columns=fs+[\"alpha\"], index=[\"VTI\"]).T\n",
    "\n",
    "ff_table = pd.concat([strat_ff, common_ff, bmark_ff], axis=1)\n",
    "print(ff_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
